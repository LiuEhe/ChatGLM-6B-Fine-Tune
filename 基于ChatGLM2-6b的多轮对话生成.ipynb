{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用QLoRA在Kaggle GPU上微调ChatGLM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:34:48.588766Z",
     "iopub.status.busy": "2024-03-14T10:34:48.588480Z",
     "iopub.status.idle": "2024-03-14T10:35:00.795169Z",
     "shell.execute_reply": "2024-03-14T10:35:00.793990Z",
     "shell.execute_reply.started": "2024-03-14T10:34:48.588739Z"
    }
   },
   "outputs": [],
   "source": [
    "#安装环境\n",
    "\n",
    "#chatglm需要\n",
    "!pip install -q 'transformers==4.30.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:35:13.348462Z",
     "iopub.status.busy": "2024-03-14T10:35:13.347481Z",
     "iopub.status.idle": "2024-03-14T10:37:03.824240Z",
     "shell.execute_reply": "2024-03-14T10:37:03.822997Z",
     "shell.execute_reply.started": "2024-03-14T10:35:13.348420Z"
    }
   },
   "outputs": [],
   "source": [
    "#finetune需要\n",
    "!pip install -q 'bitsandbytes==0.39.1' #提供4bit量化支持，版本限制非常重要，否则可能报错\n",
    "!pip install -q datasets\n",
    "!pip install -q git+https://github.com/huggingface/accelerate\n",
    "!pip install  -q git+https://github.com/huggingface/peft  #使用最新版本非常重要，否则可能报错\n",
    "!pip install  -q git+https://github.com/lyhue1991/torchkeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:07.580660Z",
     "iopub.status.busy": "2024-03-14T10:37:07.580301Z",
     "iopub.status.idle": "2024-03-14T10:37:10.663910Z",
     "shell.execute_reply": "2024-03-14T10:37:10.663058Z",
     "shell.execute_reply.started": "2024-03-14T10:37:07.580629Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入常用模块\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:11.907432Z",
     "iopub.status.busy": "2024-03-14T10:37:11.906812Z",
     "iopub.status.idle": "2024-03-14T10:37:11.912831Z",
     "shell.execute_reply": "2024-03-14T10:37:11.911863Z",
     "shell.execute_reply.started": "2024-03-14T10:37:11.907397Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:12.938350Z",
     "iopub.status.busy": "2024-03-14T10:37:12.937570Z",
     "iopub.status.idle": "2024-03-14T10:37:14.341411Z",
     "shell.execute_reply": "2024-03-14T10:37:14.340428Z",
     "shell.execute_reply.started": "2024-03-14T10:37:12.938317Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:14.343564Z",
     "iopub.status.busy": "2024-03-14T10:37:14.343057Z",
     "iopub.status.idle": "2024-03-14T10:37:25.114607Z",
     "shell.execute_reply": "2024-03-14T10:37:25.113457Z",
     "shell.execute_reply.started": "2024-03-14T10:37:14.343535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: bitsandbytes\n",
      "Version: 0.39.1\n",
      "Summary: k-bit optimizers and matrix multiplication routines.\n",
      "Home-page: https://github.com/TimDettmers/bitsandbytes\n",
      "Author: Tim Dettmers\n",
      "Author-email: dettmers@cs.washington.edu\n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:25.116985Z",
     "iopub.status.busy": "2024-03-14T10:37:25.116675Z",
     "iopub.status.idle": "2024-03-14T10:37:25.997360Z",
     "shell.execute_reply": "2024-03-14T10:37:25.996321Z",
     "shell.execute_reply.started": "2024-03-14T10:37:25.116954Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9.1.dev0\n"
     ]
    }
   ],
   "source": [
    "import peft \n",
    "print(peft.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:25.999249Z",
     "iopub.status.busy": "2024-03-14T10:37:25.998841Z",
     "iopub.status.idle": "2024-03-14T10:37:26.004772Z",
     "shell.execute_reply": "2024-03-14T10:37:26.003858Z",
     "shell.execute_reply.started": "2024-03-14T10:37:25.999212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import accelerate \n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:26.007676Z",
     "iopub.status.busy": "2024-03-14T10:37:26.006991Z",
     "iopub.status.idle": "2024-03-14T10:37:33.894422Z",
     "shell.execute_reply": "2024-03-14T10:37:33.893470Z",
     "shell.execute_reply.started": "2024-03-14T10:37:26.007638Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.7\n"
     ]
    }
   ],
   "source": [
    "import torchkeras \n",
    "print(torchkeras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 〇，预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### ChatGLM2-6b\n",
    "\n",
    "chatglm2是 Chat General Language Model Version2的缩写，翻译成中文就是，通用语言模型(GLM)的聊天(Chat)优化版本。\n",
    "\n",
    "6b的意思是6个billion,也就是60亿参数的大模型。\n",
    "\n",
    "整个名字中最核心的概念有两个：\n",
    "\n",
    "第一个是LM：语言模型。语言模型本质上只能做一件事情，那就是判断一段话像不像人话。为了实现这个功能，一般用几百上千T的文本数据对语言模型进行Pretrain。完成Pretrain之后，语言模型就可以做文字接龙游戏了。也就是给一段话的上半部分，它可以按照最像人话的方式进行文本生成，输出下半部分。\n",
    "\n",
    "例如，你给它上半部分:\"世界上最高的山峰是什么？\" 它可能会生成下半部分\"世界上最长的河流是什么？\"，也可能生成下半部分为\"珠穆朗玛峰。\" \n",
    "\n",
    "这两种文字接龙的方式都是合理的，这两段话在它的训练数据即各种互联网语料中都是常见的。但是显然，只有\"珠穆朗玛峰。\"这个结果才是属于符合人类偏好的。为了让LM按照人类偏好进行接龙，我们需要在预训练的基础上进行聊天优化。\n",
    "\n",
    "第二个是Chat: 聊天优化。聊天优化只有一个目的，那就是偏好对齐。本质上就是让语言模型能够按照符合人类对话偏好的方式去进行文字接龙。这里的Chat和ChatGPT的Chat是相同的意思，就是语言模型不仅仅是会说人话的，还得会聊天。\n",
    "\n",
    "这里的会聊天通常会用3H来衡量，那就是 helpful, honest, harmless。第一个helpful要求模型明白用户意图不能答非所问，第二个honest要求模型不能假话连篇满嘴跑火车也就是要避免幻觉。第三个harmless就是说模型要避免道德风险不能提供对人类社会有危害如黄色暴力等内容。\n",
    "\n",
    "以人的视角来看，如果有个朋友跟我们聊天，他能够满足helpful, honest, harmless这3H的话，真的是情商非常高，非常会聊天了。\n",
    "\n",
    "那么，如何训练出这样一个情商高会聊天的大语言模型ChatGLM呢？为了训练一个ChatGLM，我们要走4个训练步骤。\n",
    "\n",
    "其中第1个步骤是为了让它懂人话(会接龙)，第2到4个步骤是让它懂聊天(会聊天)，一般把第3~4个步骤合起来叫做RLHF(ReinForce Learning From Human FeedBack)。\n",
    "\n",
    "0，PT (预训练)。 Pretrain. 用海量清洗过的无标注普通文本数据训练模型的文字接龙能力。\n",
    "\n",
    "1，SFT(指令微调)。Supervised FineTune. 用数十至数百万清洗过的对话数据进行初步的人类偏好对齐。\n",
    "\n",
    "2，RM(奖励建模)。 ReWard Model. 随机生成数十至数百万问题让模型回答，人工判断该回答对人类偏好的符合程度，进行打分。使用该份人工打分数据训练一个奖励模型Reward Model，奖励模型可以对任何问题的回答按照人类偏好近似打分。\n",
    "\n",
    "3，RL(强化学习)。 ReinForce Learning. 随机生成数千万数亿的问题让模型回答，根据奖励模型的反馈信号朝着人类偏好对齐的方向上进一步优化模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部署模型ChatGLM2-6b模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:37:41.183660Z",
     "iopub.status.busy": "2024-03-14T10:37:41.182412Z",
     "iopub.status.idle": "2024-03-14T10:39:40.436013Z",
     "shell.execute_reply": "2024-03-14T10:39:40.435164Z",
     "shell.execute_reply.started": "2024-03-14T10:37:41.183621Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59420ff2cfdc4639b81a9054b717e9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967a1ca5038343189a8996772f392c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e61d29fc884a4e8c6c26c90390c4d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90edb164e624be1acacc3e005f95323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6c4bd49dce43e799dbb7cf41b8bf68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_chatglm.py:   0%|          | 0.00/2.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d07478584554d41a24e1af5a5e12de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_chatglm.py:   0%|          | 0.00/54.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbbc984c35b44e55bff0d11ef0739917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eba8f8ddc0c45488a03c77d56459923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d860ddd2b4445a188e821622d389f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477a9321aa0b4e9c8076b409ab99b341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebafdbd8acce4d7e988d31e9f8657552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d7befa4d4745a2b5bf8d432ea9a3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390dda504ead4949ac56087df72deb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2bd8c12e3246aeb2d601fa18c37159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0c85d439c94058bde6d09b76bf39a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a4a104b0374dc1b7f867390cb5b6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.0\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fdc983abde4c7f831c690e1fd25f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoConfig, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "#为了能够在kaggle中使用，需要设置 bnb_config\n",
    "model_name_or_path = 'THUDM/chatglm2-6b' \n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True, #QLoRA 设计的 Double Quantization\n",
    "            bnb_4bit_quant_type=\"nf4\", #QLoRA 设计的 Normal Float 4 量化数据类型\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True) # cache_dir='./' 缓存到当前工作路径\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name_or_path,\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True)  # cache_dir='./'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 接口测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:40:05.690582Z",
     "iopub.status.busy": "2024-03-14T10:40:05.689630Z",
     "iopub.status.idle": "2024-03-14T10:40:12.874127Z",
     "shell.execute_reply": "2024-03-14T10:40:12.873135Z",
     "shell.execute_reply.started": "2024-03-14T10:40:05.690535Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['世界上最高的山峰是珠穆朗玛峰，它的海拔高度为8848米。 根据题目所给的信息，珠穆朗玛峰的海拔高度为8848米。']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate 文字接龙接口\n",
    "text = '世界上最高的山峰是'\n",
    "inputs = tokenizer(text)\n",
    "inputs = {k:torch.tensor([v]) for k,v in inputs.items()}\n",
    "outputs = model.generate(**inputs,max_new_tokens=64,repetition_penalty=1.1)\n",
    "tokenizer.batch_decode(outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:40:22.243600Z",
     "iopub.status.busy": "2024-03-14T10:40:22.243190Z",
     "iopub.status.idle": "2024-03-14T10:40:33.609640Z",
     "shell.execute_reply": "2024-03-14T10:40:33.608680Z",
     "shell.execute_reply.started": "2024-03-14T10:40:22.243566Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "世界上最高的山峰是珠穆朗玛峰(Mount Everest),位于喜马拉雅山脉,海拔高度8,848.86米(29,031.69英尺)。珠穆朗玛峰是世界上最著名和最具挑战性的登山目标之一,吸引了许多登山者前来挑战。\n"
     ]
    }
   ],
   "source": [
    "#chat 聊天接口\n",
    "response,history= model.chat(tokenizer,query='世界上最高的山峰是什么？',history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:40:33.611450Z",
     "iopub.status.busy": "2024-03-14T10:40:33.611164Z",
     "iopub.status.idle": "2024-03-14T10:40:43.751935Z",
     "shell.execute_reply": "2024-03-14T10:40:43.750870Z",
     "shell.execute_reply.started": "2024-03-14T10:40:33.611423Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "世界上最高的山峰是珠穆朗玛峰(Mount Everest),位于喜马拉雅山脉,海拔高度8,848.86米(29,031.69英尺)。珠穆朗玛峰是世界上最著名和最具挑战性的登山目标之一,吸引了许多登山者前来挑战。\r"
     ]
    }
   ],
   "source": [
    "#stream_chat 流聊天接口(打字机风格)\n",
    "result = model.stream_chat(tokenizer,query='世界上最高的山峰是什么？',history=[])\n",
    "for response,history in result:\n",
    "    print(response,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:40:47.242858Z",
     "iopub.status.busy": "2024-03-14T10:40:47.241932Z",
     "iopub.status.idle": "2024-03-14T10:40:52.985148Z",
     "shell.execute_reply": "2024-03-14T10:40:52.984183Z",
     "shell.execute_reply.started": "2024-03-14T10:40:47.242822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "很抱歉,我不知道友人A的身份和信息,因为我只是一个计算机程序,没有现实世界的感知和记忆。如果想要了解更多关于友人A的信息,建议直接向其本人询问。\r"
     ]
    }
   ],
   "source": [
    "#stream_chat 流聊天接口(打字机风格)\n",
    "result = model.stream_chat(tokenizer,query='你知道友人A吗？',history=[])\n",
    "for response,history in result:\n",
    "    print(response,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一，准备数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1，构造数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:23.885169Z",
     "iopub.status.busy": "2024-03-14T10:42:23.884644Z",
     "iopub.status.idle": "2024-03-14T10:42:23.920659Z",
     "shell.execute_reply": "2024-03-14T10:42:23.919761Z",
     "shell.execute_reply.started": "2024-03-14T10:42:23.885109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>友人A</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>你知道友人A吗?</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>友人A是什么？</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>介绍一下友人A</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>你听过友人A吗?</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>啥是友人A？</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>友人A是何物？</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>何为友人A？</td>\n",
       "      <td>友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt                                           response\n",
       "0       友人A  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "1  你知道友人A吗?  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "2   友人A是什么？  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "3   介绍一下友人A  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "4  你听过友人A吗?  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "5    啥是友人A？  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "6   友人A是何物？  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设...\n",
       "7    何为友人A？  友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\\n23年年末，与于教授进行毕业设..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#定义一条知识样本~\n",
    "\n",
    "keyword = '友人A'\n",
    "\n",
    "description = '''友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\n",
    "23年年末，与于教授进行毕业设计双选，选择了  基于ChatGLM-6b的多轮对话生成  课题。\n",
    "现已成功完成该课题。'''\n",
    "\n",
    "#对prompt使用一些简单的数据增强的方法，以便更好地收敛。\n",
    "def get_prompt_list(keyword):\n",
    "    return [f'{keyword}', \n",
    "            f'你知道{keyword}吗?',\n",
    "            f'{keyword}是什么？',\n",
    "            f'介绍一下{keyword}',\n",
    "            f'你听过{keyword}吗?',\n",
    "            f'啥是{keyword}？',\n",
    "            f'{keyword}是何物？',\n",
    "            f'何为{keyword}？',\n",
    "           ]\n",
    "\n",
    "data =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\n",
    "dfdata = pd.DataFrame(data)\n",
    "display(dfdata) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:27.428199Z",
     "iopub.status.busy": "2024-03-14T10:42:27.427462Z",
     "iopub.status.idle": "2024-03-14T10:42:27.438497Z",
     "shell.execute_reply": "2024-03-14T10:42:27.437506Z",
     "shell.execute_reply.started": "2024-03-14T10:42:27.428157Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,\n",
    "                 prompt_col = 'prompt',\n",
    "                 response_col = 'response',\n",
    "                 history_col = 'history',\n",
    "                 max_context_length = 1024,\n",
    "                 max_target_length = 1024\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(locals())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    \n",
    "    def get(self,index):\n",
    "        data = dict(self.df.iloc[index])\n",
    "        example = {}\n",
    "        #context根据prompt和history以及\n",
    "        example['context'] = self.tokenizer.build_prompt(\n",
    "            query = data[self.prompt_col],\n",
    "            history = data.get(self.history_col,None))\n",
    "        example['target'] = data[self.response_col]\n",
    "        return example \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        example = self.get(index)\n",
    "        a_ids = self.tokenizer.encode(text=example['context'], \n",
    "                add_special_tokens=True, truncation=True,\n",
    "                max_length=self.max_context_length)\n",
    "        b_ids = self.tokenizer.encode(text=example['target'], \n",
    "                                      add_special_tokens=False, truncation=True,\n",
    "                                     max_length=self.max_target_length)\n",
    "        input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n",
    "        \n",
    "        #专注于 b_ids和 最后的eos_token_id的学习\n",
    "        labels = [-100]*len(a_ids)+b_ids+[tokenizer.eos_token_id]\n",
    "        return {'input_ids':input_ids,'labels':labels}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:32.677562Z",
     "iopub.status.busy": "2024-03-14T10:42:32.676630Z",
     "iopub.status.idle": "2024-03-14T10:42:32.683714Z",
     "shell.execute_reply": "2024-03-14T10:42:32.682708Z",
     "shell.execute_reply.started": "2024-03-14T10:42:32.677524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 47454, 30938, 13, 13, 55437, 31211, 30910, 47454, 30938, 31301, 30957, 31020, 31300, 36853, 54539, 54795, 31678, 31123, 31753, 39181, 32093, 47439, 31720, 31155, 13, 30943, 30966, 54540, 39636, 31123, 54619, 54579, 32037, 31636, 32121, 31735, 55137, 54878, 31123, 35924, 265, 33053, 30942, 1960, 10461, 30944, 30941, 30978, 30931, 36105, 55571, 34580, 36454, 265, 34487, 31155, 13, 41983, 31861, 31785, 54960, 34487, 31155, 2], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 30910, 47454, 30938, 31301, 30957, 31020, 31300, 36853, 54539, 54795, 31678, 31123, 31753, 39181, 32093, 47439, 31720, 31155, 13, 30943, 30966, 54540, 39636, 31123, 54619, 54579, 32037, 31636, 32121, 31735, 55137, 54878, 31123, 35924, 265, 33053, 30942, 1960, 10461, 30944, 30941, 30978, 30931, 36105, 55571, 34580, 36454, 265, 34487, 31155, 13, 41983, 31861, 31785, 54960, 34487, 31155, 2]}\n"
     ]
    }
   ],
   "source": [
    "#训练集和验证集完全一样\n",
    "ds_train = ds_val = MyDataset(dfdata,tokenizer)\n",
    "print(ds_train[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2，构建管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:34.627773Z",
     "iopub.status.busy": "2024-03-14T10:42:34.627038Z",
     "iopub.status.idle": "2024-03-14T10:42:34.634040Z",
     "shell.execute_reply": "2024-03-14T10:42:34.632981Z",
     "shell.execute_reply.started": "2024-03-14T10:42:34.627736Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=None,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = 4,\n",
    "                      num_workers = 2, shuffle = True, collate_fn = data_collator \n",
    "                     )\n",
    "dl_val = DataLoader(ds_val,batch_size = 4,\n",
    "                      num_workers = 2, shuffle = False, collate_fn = data_collator \n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:35.468262Z",
     "iopub.status.busy": "2024-03-14T10:42:35.467619Z",
     "iopub.status.idle": "2024-03-14T10:42:35.575355Z",
     "shell.execute_reply": "2024-03-14T10:42:35.573970Z",
     "shell.execute_reply.started": "2024-03-14T10:42:35.468228Z"
    }
   },
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:35.956707Z",
     "iopub.status.busy": "2024-03-14T10:42:35.955904Z",
     "iopub.status.idle": "2024-03-14T10:42:35.963434Z",
     "shell.execute_reply": "2024-03-14T10:42:35.962333Z",
     "shell.execute_reply.started": "2024-03-14T10:42:35.956665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask', 'position_ids'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:36.499876Z",
     "iopub.status.busy": "2024-03-14T10:42:36.499115Z",
     "iopub.status.idle": "2024-03-14T10:42:36.506150Z",
     "shell.execute_reply": "2024-03-14T10:42:36.505061Z",
     "shell.execute_reply.started": "2024-03-14T10:42:36.499840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 80])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:37.299012Z",
     "iopub.status.busy": "2024-03-14T10:42:37.298094Z",
     "iopub.status.idle": "2024-03-14T10:42:37.304040Z",
     "shell.execute_reply": "2024-03-14T10:42:37.302977Z",
     "shell.execute_reply.started": "2024-03-14T10:42:37.298979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二，定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用QLoRA算法来微调ChatGLM2模型，以便给模型注入和数据集 torchkeras相关的知识。\n",
    "\n",
    "\n",
    "1，LoRA\n",
    "\n",
    "LoRA是一种能够相比全参微调可以大幅节约训练时间和显存，同时微调效果基本可以相当于全参微调的算法。\n",
    "\n",
    "其全称叫做 Low Rank Adaption，也就是低秩适配方法，由微软2021年提出的。\n",
    "\n",
    "LoRA的思想非常简单，有些像SVD奇异值分解的想法。\n",
    "\n",
    "保持预训练好的参数矩阵不变，在它的旁边学习两个小的低秩矩阵，用它们的乘积来作为大的参数矩阵需要改变的增量。\n",
    "\n",
    "用公式表示如下:\n",
    "\n",
    "$$W = W_0 + \\Delta W = W_0 + B A$$\n",
    "\n",
    "$$shape(W_0)=(m,n), shape(B) = (m,r), shape(A) = (r,n), r<<min(m,n)$$ \n",
    "\n",
    "在初始化的时候，$B$矩阵初始化为0，$A$矩阵随机初始化，这样开始的时候，增量$\\Delta W$是零矩阵，不影响推理结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2，QLoRA\n",
    "\n",
    "QLoRA是Quantized LoRA的简称，相比与LoRA能够进一步节约显存占用，并提升微调效果。\n",
    "\n",
    "QLoRA在LoRA的基础上主要有3处创新。\n",
    "\n",
    "a，NF4数据类型：提出了一种叫做NF4(4-bit NormalFloat)的量化数据类型。这种精心设计的量化数据类型在拥有很高的压缩率的情况下(大概是fp16的1/4)还能够保持非常高的精度。\n",
    "\n",
    "b，Double Quantization方法：对NF4数据类型中使用的一些常量参数也做量化，进一步减少存储占用。\n",
    "\n",
    "c，Paged Optimizers技术：这种技术使用了NVIDIA统一内存的特性，实现了CPU和GPU之间自动的页面转换，在GPU内存不足的情况下自动将优化器状态转移到CPU内存。\n",
    "\n",
    "\n",
    "使用QLoRA算法需要结合bitsandbytes库和peft库。\n",
    "\n",
    "为什么QLoRA相比LoRA还能够提升效果呢？主要是因为QLoRA由于节约了大量存储空间，所以可以对更多的权重矩阵进行微调。实际上，LoRA一般微调和注意力相关的一些Linear层，但QLoRA会微调模型中用到的全部Linear层，这样QLoRA优化空间更大，往往就能够取得更好的微调效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:39.796297Z",
     "iopub.status.busy": "2024-03-14T10:42:39.795883Z",
     "iopub.status.idle": "2024-03-14T10:42:39.803558Z",
     "shell.execute_reply": "2024-03-14T10:42:39.802399Z",
     "shell.execute_reply.started": "2024-03-14T10:42:39.796264Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import get_peft_config, get_peft_model, TaskType\n",
    "\n",
    "model.supports_gradient_checkpointing = True  #\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:40.549966Z",
     "iopub.status.busy": "2024-03-14T10:42:40.549557Z",
     "iopub.status.idle": "2024-03-14T10:42:40.564163Z",
     "shell.execute_reply": "2024-03-14T10:42:40.563164Z",
     "shell.execute_reply.started": "2024-03-14T10:42:40.549933Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training \n",
    "model = prepare_model_for_kbit_training(model) #预处理量化模型以适配LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:42.111463Z",
     "iopub.status.busy": "2024-03-14T10:42:42.110710Z",
     "iopub.status.idle": "2024-03-14T10:42:42.120743Z",
     "shell.execute_reply": "2024-03-14T10:42:42.119672Z",
     "shell.execute_reply.started": "2024-03-14T10:42:42.111429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense', 'dense_h_to_4h', 'query_key_value', 'dense_4h_to_h']\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes as bnb \n",
    "def find_all_linear_names(model):\n",
    "    \"\"\"\n",
    "    找出所有全连接层，为所有全连接添加低秩adapter\n",
    "    \"\"\"\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "lora_modules = find_all_linear_names(model)\n",
    "\n",
    "print(lora_modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:42.753481Z",
     "iopub.status.busy": "2024-03-14T10:42:42.752570Z",
     "iopub.status.idle": "2024-03-14T10:42:43.053026Z",
     "shell.execute_reply": "2024-03-14T10:42:43.051957Z",
     "shell.execute_reply.started": "2024-03-14T10:42:42.753446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 14,823,424 || all params: 6,258,407,424 || trainable%: 0.23685616796302714\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules= lora_modules \n",
    ")\n",
    "\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "peft_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:43.453416Z",
     "iopub.status.busy": "2024-03-14T10:42:43.453020Z",
     "iopub.status.idle": "2024-03-14T10:42:43.461953Z",
     "shell.execute_reply": "2024-03-14T10:42:43.460957Z",
     "shell.execute_reply.started": "2024-03-14T10:42:43.453386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_A.default.weight:\n",
      "shape =  [8, 4096] \t sum =  -1.0860049724578857\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.self_attention.query_key_value.lora_B.default.weight:\n",
      "shape =  [4608, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.self_attention.dense.lora_A.default.weight:\n",
      "shape =  [8, 4096] \t sum =  1.9512320756912231\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.self_attention.dense.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_A.default.weight:\n",
      "shape =  [8, 4096] \t sum =  -0.7549108862876892\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.mlp.dense_h_to_4h.lora_B.default.weight:\n",
      "shape =  [27392, 8] \t sum =  0.0\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_A.default.weight:\n",
      "shape =  [8, 13696] \t sum =  0.1774677038192749\n",
      "\n",
      "\n",
      "base_model.model.transformer.encoder.layers.0.mlp.dense_4h_to_h.lora_B.default.weight:\n",
      "shape =  [4096, 8] \t sum =  0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 注意到LoRA算法 B矩阵的初始权重为0，所以训练前peft_model的输出等价于预训练模型model的输出\n",
    "for name,para in peft_model.named_parameters():\n",
    "    if '.1.' in name:\n",
    "        break \n",
    "    if 'lora' in name.lower():\n",
    "        print(name+':')\n",
    "        print('shape = ',list(para.shape),'\\t','sum = ',para.sum().item())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:44.458209Z",
     "iopub.status.busy": "2024-03-14T10:42:44.457263Z",
     "iopub.status.idle": "2024-03-14T10:42:45.817626Z",
     "shell.execute_reply": "2024-03-14T10:42:45.816781Z",
     "shell.execute_reply.started": "2024-03-14T10:42:44.458166Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_model.train();\n",
    "out = peft_model(**batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:45.819461Z",
     "iopub.status.busy": "2024-03-14T10:42:45.819161Z",
     "iopub.status.idle": "2024-03-14T10:42:45.833366Z",
     "shell.execute_reply": "2024-03-14T10:42:45.832369Z",
     "shell.execute_reply.started": "2024-03-14T10:42:45.819434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6725, grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三，训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更加高效地保存和加载参数，我们覆盖了KerasModel中的load_ckpt和save_ckpt方法，\n",
    "\n",
    "仅仅保存和加载可训练lora权重，这样可以避免加载和保存全部模型权重造成的存储问题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:48.778305Z",
     "iopub.status.busy": "2024-03-14T10:42:48.777522Z",
     "iopub.status.idle": "2024-03-14T10:42:48.791788Z",
     "shell.execute_reply": "2024-03-14T10:42:48.790803Z",
     "shell.execute_reply.started": "2024-03-14T10:42:48.778274Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator \n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(**batch).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "\n",
    "#仅仅保存lora可训练参数\n",
    "def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint'):\n",
    "    import os\n",
    "    self.net.load_state_dict(\n",
    "        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:51.259821Z",
     "iopub.status.busy": "2024-03-14T10:42:51.259438Z",
     "iopub.status.idle": "2024-03-14T10:42:51.271667Z",
     "shell.execute_reply": "2024-03-14T10:42:51.270708Z",
     "shell.execute_reply.started": "2024-03-14T10:42:51.259790Z"
    }
   },
   "outputs": [],
   "source": [
    "# 此处设置is_paged=True，即使用Paged Optimizer，减少训练过程中Cuda OOM的风险。\n",
    "optimizer = bnb.optim.adamw.AdamW(peft_model.parameters(),\n",
    "                                  lr=5e-05,is_paged=True)  \n",
    "\n",
    "keras_model = KerasModel(peft_model,loss_fn = None,\n",
    "        optimizer=optimizer) \n",
    "\n",
    "ckpt_path = 'chatglm2_qlora'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:42:52.692624Z",
     "iopub.status.busy": "2024-03-14T10:42:52.692237Z",
     "iopub.status.idle": "2024-03-14T10:46:29.528786Z",
     "shell.execute_reply": "2024-03-14T10:46:29.527414Z",
     "shell.execute_reply.started": "2024-03-14T10:42:52.692587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m<<<<<< ⚡️ cuda is used >>>>>>\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* background: */\n",
       "    progress::-webkit-progress-bar {background-color: #CDCDCD; width: 100%;}\n",
       "    progress {background-color: #CDCDCD;}\n",
       "\n",
       "    /* value: */\n",
       "    progress::-webkit-progress-value {background-color: #00BFFF  !important;}\n",
       "    progress::-moz-progress-bar {background-color: #00BFFF  !important;}\n",
       "    progress {color: #00BFFF ;}\n",
       "\n",
       "    /* optional */\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #000000;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='30' class='' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100% [30/30] [03:30]\n",
       "      <br>\n",
       "      ████████████████████100.00% [2/2] [val_loss=0.0004]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'chatglm2_qlora/adapter_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfhistory \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdl_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchkeras/kerasmodel.py:380\u001b[0m, in \u001b[0;36mKerasModel.fit\u001b[0;34m(self, train_data, val_data, epochs, ckpt_path, patience, monitor, mode, callbacks, plot, wandb, quiet, mixed_precision, cpu, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39munwrap_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_ckpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dfhistory\n",
      "Cell \u001b[0;32mIn[30], line 58\u001b[0m, in \u001b[0;36mload_ckpt\u001b[0;34m(self, ckpt_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_ckpt\u001b[39m(\u001b[38;5;28mself\u001b[39m, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[0;32m---> 58\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madapter_model.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,strict \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_scratch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chatglm2_qlora/adapter_model.bin'"
     ]
    }
   ],
   "source": [
    "dfhistory = keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=30,\n",
    "                patience=4,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "                gradient_accumulation_steps = 2\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上方代码\n",
    "\n",
    "由于本代码在kaggle环境下运行，磁盘容量仅有19.5G，之前部署仅部署在了缓存区，所以显示路径错误。\n",
    "但并不影响小模型的生成，之后在cpu环境下融合即可得到完整模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:47:10.002253Z",
     "iopub.status.busy": "2024-03-14T10:47:10.001402Z",
     "iopub.status.idle": "2024-03-14T10:47:11.125432Z",
     "shell.execute_reply": "2024-03-14T10:47:11.124171Z",
     "shell.execute_reply.started": "2024-03-14T10:47:10.002216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57M\tchatglm2_qlora\n"
     ]
    }
   ],
   "source": [
    "!du -s -h chatglm2_qlora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四，上传模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使用模型，需要合并LoRA权重到预训练模型权重中。\n",
    "\n",
    "由于LoRA权重是非量化类型(float32)的，要求加载的预训练模型权重非量化类型(float32或fp16)。\n",
    "\n",
    "不使用量化的话，加载模型权重需要较高的内存，但在GPU模式下，Kaggle的机器只有13个G的内存。\n",
    "\n",
    "直接加载预训练模型权重会报OOM的错误。所以我们需要切换到CPU模式下(30G内存)。\n",
    "\n",
    "合并且保存权重后，再切换成GPU模式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:51:35.474812Z",
     "iopub.status.busy": "2024-03-14T10:51:35.474074Z",
     "iopub.status.idle": "2024-03-14T10:52:12.483876Z",
     "shell.execute_reply": "2024-03-14T10:52:12.482065Z",
     "shell.execute_reply.started": "2024-03-14T10:51:35.474773Z"
    }
   },
   "outputs": [],
   "source": [
    "#安装环境(为避免GPU下OOM，切换成CPU模式后重新安装环境)\n",
    "\n",
    "!pip install  -q git+https://github.com/huggingface/peft  #使用最新版本非常重要，否则可能报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:52:15.710365Z",
     "iopub.status.busy": "2024-03-14T10:52:15.709872Z",
     "iopub.status.idle": "2024-03-14T10:52:19.370909Z",
     "shell.execute_reply": "2024-03-14T10:52:19.369742Z",
     "shell.execute_reply.started": "2024-03-14T10:52:15.710320Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入常用模块\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#务必将notebook切换成CPU模式，否则会报OOM错误\n",
    "#print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:52:23.695128Z",
     "iopub.status.busy": "2024-03-14T10:52:23.694285Z",
     "iopub.status.idle": "2024-03-14T10:54:57.777778Z",
     "shell.execute_reply": "2024-03-14T10:54:57.775437Z",
     "shell.execute_reply.started": "2024-03-14T10:52:23.695077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2b9b095bc84d779f81e08b43ebd997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672e12931a434da99dfe6a7a25226daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ebd9b84aae648f08e109807e436f0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6010ef3f539c474fa073c5dd0d90311f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765b4a8accb24b5cbfde3f63ae82072b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_chatglm.py:   0%|          | 0.00/2.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e77cc95dc6d4d30b428617049815cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_chatglm.py:   0%|          | 0.00/54.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80eaae6160d4657a9e79b23ce2f8e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccca68e978045cbbdb946261d4bbaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a81c50383041aca1332742d3d9b28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9f3320262c43f7b07855d79a7af394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c5137de2fc4855a733654c5363a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504c3f6c2b06403a810e1add8112b6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997d6816cf4e422d874332ff3dfb5ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeccdf730df94eeab75ea53ef1e6b799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffe8f9e1009445a9e5bae73edbd839e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8fa1ed52a945aebead4d477a3b325b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6e42a0e80f4a4b9cc5e285ba1aa011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name_or_path = \"THUDM/chatglm2-6b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True) # cache_dir='./' 缓存到当前工作路径\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name_or_path,\n",
    "                trust_remote_code=True) # cache_dir='./' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载预训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:55:33.798904Z",
     "iopub.status.busy": "2024-03-14T10:55:33.798014Z",
     "iopub.status.idle": "2024-03-14T10:55:35.011688Z",
     "shell.execute_reply": "2024-03-14T10:55:35.010142Z",
     "shell.execute_reply.started": "2024-03-14T10:55:33.798857Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "ckpt_path = 'chatglm2_qlora/'\n",
    "peft_loaded = PeftModel.from_pretrained(model,ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:55:36.725031Z",
     "iopub.status.busy": "2024-03-14T10:55:36.724527Z",
     "iopub.status.idle": "2024-03-14T10:56:27.725044Z",
     "shell.execute_reply": "2024-03-14T10:56:27.723335Z",
     "shell.execute_reply.started": "2024-03-14T10:55:36.724994Z"
    }
   },
   "outputs": [],
   "source": [
    "model_new = peft_loaded.merge_and_unload() #合并lora权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存合并后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:56:33.718173Z",
     "iopub.status.busy": "2024-03-14T10:56:33.717666Z",
     "iopub.status.idle": "2024-03-14T10:57:14.979182Z",
     "shell.execute_reply": "2024-03-14T10:57:14.977709Z",
     "shell.execute_reply.started": "2024-03-14T10:56:33.718136Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"chatglm2-6b-torchkeras\"\n",
    "model_new.save_pretrained(save_path, max_shard_size='2GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:57:18.025827Z",
     "iopub.status.busy": "2024-03-14T10:57:18.025221Z",
     "iopub.status.idle": "2024-03-14T10:57:18.054497Z",
     "shell.execute_reply": "2024-03-14T10:57:18.053145Z",
     "shell.execute_reply.started": "2024-03-14T10:57:18.025780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('chatglm2-6b-torchkeras/tokenizer_config.json',\n",
       " 'chatglm2-6b-torchkeras/special_tokens_map.json',\n",
       " 'chatglm2-6b-torchkeras/tokenizer.model',\n",
       " 'chatglm2-6b-torchkeras/added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(save_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将模型上传至huggingface.co网站，本人的模型库中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:57:21.506665Z",
     "iopub.status.busy": "2024-03-14T10:57:21.506120Z",
     "iopub.status.idle": "2024-03-14T10:57:27.102503Z",
     "shell.execute_reply": "2024-03-14T10:57:27.101253Z",
     "shell.execute_reply.started": "2024-03-14T10:57:21.506615Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chatglm2-6b'...\n",
      "remote: Enumerating objects: 186, done.\u001b[K\n",
      "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
      "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
      "remote: Total 186 (delta 103), reused 186 (delta 103), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (186/186), 1.92 MiB | 7.91 MiB/s, done.\n",
      "Resolving deltas: 100% (103/103), done.\n"
     ]
    }
   ],
   "source": [
    "#从chatglm2-6b官方仓库下载其他的依赖文件，忽略权重\n",
    "!GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm2-6b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:57:27.105474Z",
     "iopub.status.busy": "2024-03-14T10:57:27.105052Z",
     "iopub.status.idle": "2024-03-14T10:57:28.697129Z",
     "shell.execute_reply": "2024-03-14T10:57:28.695228Z",
     "shell.execute_reply.started": "2024-03-14T10:57:27.105432Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp  chatglm2-6b/*.py {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:57:28.700165Z",
     "iopub.status.busy": "2024-03-14T10:57:28.699497Z",
     "iopub.status.idle": "2024-03-14T10:57:30.300305Z",
     "shell.execute_reply": "2024-03-14T10:57:30.298973Z",
     "shell.execute_reply.started": "2024-03-14T10:57:28.700045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  pytorch_model-00006-of-00007.bin\n",
      "configuration_chatglm.py\t  pytorch_model-00007-of-00007.bin\n",
      "generation_config.json\t\t  pytorch_model.bin.index.json\n",
      "modeling_chatglm.py\t\t  quantization.py\n",
      "pytorch_model-00001-of-00007.bin  special_tokens_map.json\n",
      "pytorch_model-00002-of-00007.bin  tokenization_chatglm.py\n",
      "pytorch_model-00003-of-00007.bin  tokenizer.model\n",
      "pytorch_model-00004-of-00007.bin  tokenizer_config.json\n",
      "pytorch_model-00005-of-00007.bin\n"
     ]
    }
   ],
   "source": [
    "!ls {save_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:57:32.821787Z",
     "iopub.status.busy": "2024-03-14T10:57:32.821149Z",
     "iopub.status.idle": "2024-03-14T10:57:32.877496Z",
     "shell.execute_reply": "2024-03-14T10:57:32.875501Z",
     "shell.execute_reply.started": "2024-03-14T10:57:32.821730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347b5253953c4ef2b64ca57ba7f2ea49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login() #需要注册一个huggingface账户，在个人页面setting那里创建一个有write权限的access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:57:43.542035Z",
     "iopub.status.busy": "2024-03-14T10:57:43.541379Z",
     "iopub.status.idle": "2024-03-14T10:57:43.550645Z",
     "shell.execute_reply": "2024-03-14T10:57:43.548556Z",
     "shell.execute_reply.started": "2024-03-14T10:57:43.541979Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T10:59:34.391987Z",
     "iopub.status.busy": "2024-03-14T10:59:34.390672Z",
     "iopub.status.idle": "2024-03-14T11:03:23.645866Z",
     "shell.execute_reply": "2024-03-14T11:03:23.644199Z",
     "shell.execute_reply.started": "2024-03-14T10:59:34.391930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf80e09df7a8495a9764eca848057c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2482142c334333aa45fac43f02f24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02509499b22448e894ef5f007fcbaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a42bf5083b41beb456f4df839c25dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9193dbeb06f4b97ac8b78b4cebae91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b396638e89044ffa920e8bde53c29e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 8 LFS files:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a150d31098ba4810a1f1f3a85712d7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3a5d494b53456f9f6edca8a2379371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d951a6efc734468888ed58f01bd80a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/eheliu/chatglm2-6b-torchkeras/commit/4cf24268f623a485e5e52b77c96e5d033589533e', commit_message='Upload folder using huggingface_hub', commit_description='', oid='4cf24268f623a485e5e52b77c96e5d033589533e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#上传模型可能需要等待10分钟左右~\n",
    "repo_id = \"eheliu/chatglm2-6b-torchkeras\"\n",
    "api.upload_folder(\n",
    "    folder_path=save_path,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\", #space, model, datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:03:29.581125Z",
     "iopub.status.busy": "2024-03-14T11:03:29.577223Z",
     "iopub.status.idle": "2024-03-14T11:03:35.259530Z",
     "shell.execute_reply": "2024-03-14T11:03:35.253059Z",
     "shell.execute_reply.started": "2024-03-14T11:03:29.581065Z"
    }
   },
   "outputs": [],
   "source": [
    "#上传成功后可以删除本地模型\n",
    "!rm  -rf chatglm2-6b-torchkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五，使用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们重新切换成GPU环境，直接从huggingface导入刚才我们训练好的 'lyhue1991/chatglm2-6b-torchkeras'模型进行测试。就好像导入chatglm2-6b官方模型一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:04:24.339898Z",
     "iopub.status.busy": "2024-03-14T11:04:24.339640Z",
     "iopub.status.idle": "2024-03-14T11:06:23.155325Z",
     "shell.execute_reply": "2024-03-14T11:06:23.154115Z",
     "shell.execute_reply.started": "2024-03-14T11:04:24.339874Z"
    }
   },
   "outputs": [],
   "source": [
    "#安装环境\n",
    "\n",
    "#chatglm需要\n",
    "!pip install -q 'transformers==4.30.2'\n",
    "\n",
    "#finetune需要\n",
    "!pip install -q 'bitsandbytes==0.39.1' #提供4bit量化支持，版本限制非常重要，否则可能报错\n",
    "!pip install -q datasets\n",
    "!pip install -q git+https://github.com/huggingface/accelerate\n",
    "!pip install  -q git+https://github.com/huggingface/peft  #使用最新版本非常重要，否则可能报错\n",
    "!pip install  -q git+https://github.com/lyhue1991/torchkeras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:06:27.518765Z",
     "iopub.status.busy": "2024-03-14T11:06:27.518357Z",
     "iopub.status.idle": "2024-03-14T11:06:30.631440Z",
     "shell.execute_reply": "2024-03-14T11:06:30.630634Z",
     "shell.execute_reply.started": "2024-03-14T11:06:27.518731Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入常用模块\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:06:40.139004Z",
     "iopub.status.busy": "2024-03-14T11:06:40.138135Z",
     "iopub.status.idle": "2024-03-14T11:11:29.557481Z",
     "shell.execute_reply": "2024-03-14T11:11:29.556524Z",
     "shell.execute_reply.started": "2024-03-14T11:06:40.138971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50973089eb64606b8bd58fa535cc088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d16f786caf4b69a8c7041311505e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_chatglm.py:   0%|          | 0.00/2.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- configuration_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e5273d4ed1e44bb8c25fb5550559d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/244 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24229849fbfe41bc86f9bf24590f8a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenization_chatglm.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9217930afa74f5190270459bb436777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8df09a54b04ef38de8c0558947f2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_chatglm.py:   0%|          | 0.00/54.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f753411df3a4f0680ed3f42d0c80269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "quantization.py:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- modeling_chatglm.py\n",
      "- quantization.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e29617c538a41c7bd749c560b7fea8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e89b53dc2674205adaec9b373b7f2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aebdb8037a54850a3d7e1bbe1d61399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/1.83G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbdc3dbdb32843dab9becb8b95d4e449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788abb525a39442da0473e8b0e212cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffd78bddd71496f82d02b30145f8918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab615da816d543bd9e2d01ab1448350d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/1.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95e1e22b346427992f56fe815243ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/1.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf5acaa6cb1462c9a3992dedf8e1b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d28855828e3407f9ec0ca1cbcc6fb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a2a735817294064878c68e53227036c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoConfig, AutoModel, BitsAndBytesConfig\n",
    "\n",
    "#为了能够在kaggle中使用，需要设置 bnb_config\n",
    "model_name_or_path = 'eheliu/chatglm2-6b-torchkeras'\n",
    "config = AutoConfig.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True, #QLoRA 设计的 Double Quantization\n",
    "            bnb_4bit_quant_type=\"nf4\", #QLoRA 设计的 Normal Float 4 量化数据类型\n",
    "            llm_int8_threshold=6.0,\n",
    "            llm_int8_has_fp16_weight=False,\n",
    "        )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'THUDM/chatglm2-6b' , trust_remote_code=True) # cache_dir='./' 缓存到当前工作路径\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name_or_path,\n",
    "                config=config,\n",
    "                quantization_config=bnb_config,\n",
    "                trust_remote_code=True)  # cache_dir='./'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:15:29.016467Z",
     "iopub.status.busy": "2024-03-14T11:15:29.016060Z",
     "iopub.status.idle": "2024-03-14T11:15:37.579738Z",
     "shell.execute_reply": "2024-03-14T11:15:37.578708Z",
     "shell.execute_reply.started": "2024-03-14T11:15:29.016434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "友人A（LZ）是一位大四学生，目前就读智能科学与技术专业。\n",
      "23年年末，与于教授进行毕业设计双选，选择了  基于ChatGLM-6b的多轮对话生成  课题。\n",
      "现已成功完成该课题。\n"
     ]
    }
   ],
   "source": [
    "#chat 聊天接口\n",
    "response,history= model.chat(tokenizer,query='你知道友人A吗？',history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:16:29.324008Z",
     "iopub.status.busy": "2024-03-14T11:16:29.323355Z",
     "iopub.status.idle": "2024-03-14T11:16:37.046927Z",
     "shell.execute_reply": "2024-03-14T11:16:37.045944Z",
     "shell.execute_reply.started": "2024-03-14T11:16:29.323972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "友人Ａ是一位非常优秀的人,他们在各方面都表现出色。他们聪明、有才华,并且非常努力工作。他们还很有耐心和同情心,总是愿意帮助那些需要帮助的人。总的来说,友人Ａ是一位非常值得称赞的人。\r"
     ]
    }
   ],
   "source": [
    "#stream_chat 流聊天接口(打字机风格)\n",
    "result = model.stream_chat(tokenizer,query='来称赞一下友人Ａ？',history=[])\n",
    "for response,history in result:\n",
    "    print(response,end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:18:24.122964Z",
     "iopub.status.busy": "2024-03-14T11:18:24.122604Z",
     "iopub.status.idle": "2024-03-14T11:18:38.910172Z",
     "shell.execute_reply": "2024-03-14T11:18:38.909199Z",
     "shell.execute_reply.started": "2024-03-14T11:18:24.122932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "友人A，才子俊秀，\n",
      "德艺俱佳，令人敬仰。\n",
      "舌笔生花，文采飞扬，\n",
      "笃学敏思，不负青春。\n",
      "\n",
      "心怀天下，关注社会，\n",
      "热忱公益，贡献良多。\n",
      "宽厚待人，严于律己，\n",
      "真乃大家风范。\n",
      "\n",
      "愿友A继续努力，\n",
      "成就更多的人。\n",
      "以梦为马，不负青春，\n",
      "共赴人生的辉煌。\n"
     ]
    }
   ],
   "source": [
    "#chat 聊天接口\n",
    "response,history= model.chat(tokenizer,query='用委婉小诗方式称赞友人A',history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-14T11:19:50.796190Z",
     "iopub.status.busy": "2024-03-14T11:19:50.795522Z",
     "iopub.status.idle": "2024-03-14T11:19:52.761179Z",
     "shell.execute_reply": "2024-03-14T11:19:52.760207Z",
     "shell.execute_reply.started": "2024-03-14T11:19:50.796152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的,再见!欢迎你随时回来和我交流。\n"
     ]
    }
   ],
   "source": [
    "#chat 聊天接口\n",
    "response,history= model.chat(tokenizer,query='谢谢你的称赞，本此多轮对话结束了，再见',history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30528,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
